{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Novel Coronavirus COVID-19 (2019-nCoV) Unpivoted Data\n",
    "\n",
    "The following script takes data from the repository of the 2019 Novel Coronavirus Visual Dashboard operated by \n",
    "the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). It will apply necessary \n",
    "cleansing/reformatting to make it use in traditional relational databases and data visualization tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pygsheets\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pycountry\n",
    "from copy import deepcopy\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded directly from Johns Hopkins git repository, located at: https://github.com/CSSEGISandData/COVID-19. Their repository has three different CSV files for `confirmed`, `deaths` and `recovered` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv\",keep_default_na=False)\n",
    "deaths = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv\",keep_default_na=False)\n",
    "recovered = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv\",keep_default_na=False)\n",
    "\n",
    "confirmed['Case_Type'] = 'Confirmed'\n",
    "deaths['Case_Type'] = 'Deaths'\n",
    "recovered['Case_Type'] = 'Recovered'\n",
    "\n",
    "key_columns = ['Country/Region','Province/State','Lat','Long','Case_Type']\n",
    "\n",
    "data = [confirmed, deaths, recovered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset stores the number of `Cases` for a given day in columns. \n",
    "This is not useful for reporting, thus we move these date columns to rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(df):\n",
    "    # unpivot all non-key columns\n",
    "    melted = df.melt(id_vars=key_columns, var_name='Date', value_name = 'Cases')\n",
    "    # change our new Date field to Date type\n",
    "    melted['Date']= pd.to_datetime(melted['Date']) \n",
    "    \n",
    "    return melted\n",
    "\n",
    "unpivoted_data = list(map(unpivot, data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality \n",
    "\n",
    " 1. Replace empty values in cases to zero\n",
    " 2. Maitain consistent country naming (see: https://github.com/CSSEGISandData/COVID-19/issues/396)\n",
    " 3. After renaming countries, aggregate values values for one country/province per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop incorrect county/state data\n",
    "\n",
    "for df in unpivoted_data:\n",
    "    stateBeforeMarch9th = df[ (df['Date'] <= '2020-03-09') & (df['Country/Region'] == 'US') & (df['Province/State'].str.contains(',') == False) ].index\n",
    "    countryAfterMarch10th = df[ (df['Date'] > '2020-03-09') & (df['Country/Region'] == 'US') & df['Province/State'].str.contains(',') ].index\n",
    "\n",
    "    df.drop(stateBeforeMarch9th, inplace=True)\n",
    "    df.drop(countryAfterMarch10th, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivisions = {i.name: i.code for i in pycountry.subdivisions.get(country_code=\"US\")}\n",
    "abbreviations = {subdivisions[k]: k for k in subdivisions}\n",
    "locality_replacements = {\"Washington, D.C.\": \"District of Columbia\"}\n",
    "\n",
    "for idx, df in enumerate(unpivoted_data):\n",
    "    unpivoted_data[idx].replace(locality_replacements, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_US_geography(row):\n",
    "    county, state = row[\"Province/State\"].split(\", \")\n",
    "    state.replace(\"D.C.\", \"DC\")\n",
    "    row[\"Province/State\"] = abbreviations[\"US-\" + state.strip()]\n",
    "    return row\n",
    "        \n",
    "def resolve_geography_df(df):\n",
    "    return df.apply(lambda row: resolve_US_geography(row) if row['Country/Region'] == 'US' and row[\"Province/State\"] not in list(subdivisions.keys()) and \", \" in row[\"Province/State\"] else row, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in unpivoted_data:\n",
    "    df = resolve_geography_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_names = {\n",
    "    \"Holy See\": \"Vatican City\",\n",
    "    \"Hong Kong SAR\": \"Hong Kong\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Macao SAR\": \"Macau\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Republic of Moldova\": \"Moldova\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Saint Martin\": \"St. Martin\",\n",
    "    \"Taipei and environs\": \"Taiwan\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"occupied Palestinian territory\": \"Palestine\",\n",
    "}\n",
    "\n",
    "\n",
    "for idx,df in enumerate(unpivoted_data):\n",
    "    df[\"Country/Region\"] = df[\"Country/Region\"].replace(changed_names)\n",
    "    df[\"Cases\"] = df[\"Cases\"].replace('',0).astype(int)\n",
    "        \n",
    "    unpivoted_data[idx] = df.groupby(by=[\"Country/Region\",\"Province/State\",\"Date\",\"Case_Type\"], as_index=False) \\\n",
    "        .agg({\"Cases\": \"sum\", \"Long\": \"first\", \"Lat\": \"first\"})\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the data by primary keys and `Date`, to make sure we can add a `Differences` column easily. \n",
    "\n",
    "As `Cases` are actual snapshots (running numbers), to know what was the change since the previous day we introduce a new column called `Differences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = list( map(lambda df: df.sort_values(by=key_columns + ['Date'], ascending=True), unpivoted_data) )\n",
    "\n",
    "#sorted_data[0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Difference` is today's `Cases` minus yesterday's `Cases` for each region/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in sorted_data:\n",
    "    df[\"Difference\"] = df[\"Cases\"] - df.groupby( key_columns )[\"Cases\"].shift(1, fill_value = 0) \n",
    "\n",
    "concated = pd.concat(sorted_data)\n",
    "\n",
    "#concated.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concated = concated[concated['Date'] <= '2020-03-09' ]\n",
    "\n",
    "concated['Date'].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to show the number of active cases. In our definition, `Active` is calculated as:\n",
    "\n",
    "```\n",
    "Active = Confirmed - Deaths - Recovered\n",
    "```\n",
    "\n",
    "As a first step, we merge the different type of cases into a single line for each `Country/Province/Date` keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = concated[concated[\"Case_Type\"].eq(\"Confirmed\")]\n",
    "deaths = concated[concated[\"Case_Type\"].eq(\"Deaths\")]\n",
    "recovered = concated[concated[\"Case_Type\"].eq(\"Recovered\")]\n",
    "\n",
    "active = confirmed  \\\n",
    "        .merge(deaths, validate= \"one_to_one\", suffixes =[\"\",\"_d\"], on=[\"Country/Region\",\"Province/State\",\"Date\"]) \\\n",
    "        .merge(recovered, validate= \"one_to_one\", suffixes =[\"\",\"_r\"], on= [\"Country/Region\",\"Province/State\",\"Date\"])\n",
    "\n",
    "#active.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apply the calculations both for `Cases` and `Difference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active[\"Case_Type\"] = 'Active'\n",
    "active[\"Cases\"] = active[\"Cases\"] - active[\"Cases_r\"] - active[\"Cases_d\"]\n",
    "active[\"Difference\"] = active[\"Difference\"] - active[\"Difference_r\"] - active[\"Difference_d\"]\n",
    "\n",
    "#active.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then merge the `Active` dataset with the original one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([concated,active], join=\"inner\")\n",
    "\n",
    "data[\"Case_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Province/State\"] == \"California\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save the file locally, we add the `Last_Update_Date` in `UTC` time zone.\n",
    "\n",
    "### Writing local file: `JHU_COVID-19.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Last_Update_Date\"] = datetime.utcnow()\n",
    "data.to_csv(\"./JHU_COVID-19.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
