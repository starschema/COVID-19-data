# This is a basic workflow to help you get started with Actions

name: Airflow/DAG

# Controls when the action will run. Triggers the workflow on push or pull request 
# events but only for the master branch
on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  airflow:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
      
    - name: Set up Python
      uses: actions/setup-python@v1
      with:
        python-version: '3.7' 

    - uses: actions/setup-java@v1
      with:
        java-version: '9.0.4' 

    - name: Run pip install
      run: |
        pip install -r airflow-requirements.txt
        pip install -r requirements.txt
        
    - name: Configure Airflow
      run: |
        export AIRFLOW_HOME=${{ GITHUB.workspace }}
        airflow upgradedb
        airflow variables -s ENVIRONMENT ci
        airflow variables -s S3_BUCKET test-covid19
        airflow variables -s AWS_ACCESS_KEY_ID ${{ secrets.AWS_ACCESS_KEY_ID }}
        airflow variables -s AWS_SECRET_ACCESS_KEY ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        airflow list_dags

    - name: Run DAG tests
      run: |
        export AIRFLOW_HOME=${{ GITHUB.workspace }}
        airflow test etl_JHU_COVID-19 execute_notebook 2020-01-01   
        airflow test etl_JHU_COVID-19 upload_to_s3 2020-01-01   
        airflow test etl_PCM_DPS_COVID19 execute_notebook 2020-01-01   
        airflow test etl_PCM_DPS_COVID19 upload_to_s3 2020-01-01   
        airflow test etl_CT_US_COVID_TESTS execute_notebook 2020-01-01   
        airflow test etl_CT_US_COVID_TESTS upload_to_s3 2020-01-01   
        airflow test etl_WHO_SITUATION_REPORTS execute_notebook 2020-01-01   
        airflow test etl_WHO_SITUATION_REPORTS upload_to_s3 2020-01-01   
        airflow test etl_VH_CAN_DETAILED execute_notebook 2020-01-01   
        airflow test etl_VH_CAN_DETAILED upload_to_s3 2020-01-01   
        airflow test etl_METADATA execute_notebook 2020-01-01   
        airflow test etl_METADATA upload_to_s3 2020-01-01   
