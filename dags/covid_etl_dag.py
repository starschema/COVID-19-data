import airflow
import boto3
from airflow import DAG
from datetime import datetime, timedelta, time
from airflow.utils.dates import days_ago
from airflow.operators.python_operator import PythonOperator
from airflow.contrib.operators.snowflake_operator import SnowflakeOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.models import Variable
import os
import papermill as pm


def create_dag(dag_id, args):

    dag = DAG(
        dag_id=dag_id,
        default_args=args,
        schedule_interval=None,
        dagrun_timeout=timedelta(minutes=60)
    )

    basename = args.get('basename')
    # notebook name without extension, for example "JHU_COVID-19"

    notebook_file = Variable.get('notebooks_dir') + '/' + basename + ".ipynb"
    # notebook file, this will be executed, for example: /home/ec2-user/COVID-19-data/notebooks/JHU_COVID-19.ipynb

    notebook_out_file = Variable.get('notebooks_out_file_dir') + '/' + basename + '_out.ipynb'
    # notebook execution will generate a new notebook, for example: /home/ec2-user/notebooks_output/JHU_COVID-19_out.ipynb

    output_file = Variable.get("notebook_output_dir") + "/" + basename + '.csv'
    # the csv file location which wil be generated by the notebook, for example: /home/ec2-user/output/JHU_COVID-19.csv

    s3_file_name = args.get('basename') + '.csv'

    with dag:
        start = DummyOperator(
            task_id='start',
            dag=dag
        )

        def clean_generated_files():
            if os.path.exists(notebook_out_file):
                os.remove(notebook_out_file)
            if os.path.exists(output_file):
                os.remove(output_file)

        def execute_notebook():
            pm.execute_notebook(
                input_path=notebook_file,
                output_path=notebook_out_file,
                parameters=dict(),
                log_output=True,
                report_mode=True
            )
            return

        def upload_to_s3():
            """Upload a file to an S3 bucket

            :param basename
            :return: True if file was uploaded, else False
            """
            # Upload the file
            s3_client = boto3.client('s3', aws_access_key_id=Variable.get("AWS_ACCESS_KEY_ID"),
                                     aws_secret_access_key=Variable.get("AWS_SECRET_ACCESS_KEY"))
            response = s3_client.upload_file(output_file,
                                             Variable.get("prod_bucket"),
                                             s3_file_name)

            return response

        def upload_to_snowflake(task_id):

            insert_st = f'copy into {basename.replace("-","_")} from @COVID_PROD/{basename}.csv file_format = (type = "csv" field_delimiter = ","  FIELD_OPTIONALLY_ENCLOSED_BY=\'"\' skip_header = 1)'

            create_insert_task = SnowflakeOperator(
                task_id=task_id,
                sql=insert_st,
                snowflake_conn_id="OJ10999_COVID19",
            )

            return create_insert_task

        def create_dynamic_etl(task_id, callable_function):
            task = PythonOperator(
                task_id=task_id,
                python_callable=callable_function,
                dag=dag,
            )
            return task

        end = DummyOperator(
            task_id='end',
            dag=dag)

        cleanup_output_folder_task = create_dynamic_etl('{}-cleanup'.format(file), clean_generated_files)

        execute_notebook_task = create_dynamic_etl('{}-execute_notebook'.format(file), execute_notebook)

        upload_to_s3_task = create_dynamic_etl('{}-uploadDataToS3'.format(file), upload_to_s3)

        upload_to_snowflake_task = upload_to_snowflake('{}-uploadDataToSnowflake'.format(file))

        start >> cleanup_output_folder_task
        cleanup_output_folder_task >> execute_notebook_task
        execute_notebook_task >> upload_to_s3_task
        upload_to_s3_task >> upload_to_snowflake_task
        upload_to_snowflake_task >> end

        return dag


# build a dag for each number in range(10)
i = 0
for file in os.listdir(Variable.get("notebooks_dir")):
    if file.startswith("."):
        continue
    filename_without_extension = os.path.splitext(file)[0]
    dag_id = 'etl_{}'.format(str(filename_without_extension))

    default_args = {'owner': 'airflow',
                    'start_date': days_ago(2),
                    'basename': filename_without_extension
                    }

    dag_number = i
    i += 1
    globals()[dag_id] = create_dag(dag_id, default_args)
