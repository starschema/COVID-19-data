{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Novel Coronavirus (SARS-CoV-2) and COVID-19 Unpivoted Data\n",
    "\n",
    "The following script takes data from the repository of the 2019 Novel Coronavirus Visual Dashboard operated by Johns Hopkins University's Center for Systems Science and Engineering (JHU CSSE). It will apply necessary cleansing/reformatting to make it use in traditional relational databases and data visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pycountry\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "output_folder = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded directly from Johns Hopkins git repository, located at: https://github.com/CSSEGISandData/COVID-19. Their repository has three different CSV files – one each for `confirmed`, `deaths` and `recovered` data. The data is keyed into an array of `pandas` `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pd.read_csv(\"https://s3-us-west-1.amazonaws.com/starschema.covid/time_series_covid19_confirmed_global.csv\",thousands=r',',keep_default_na=False)\n",
    "deaths = pd.read_csv(\"https://s3-us-west-1.amazonaws.com/starschema.covid/time_series_covid19_deaths_global.csv\",thousands=r',',keep_default_na=False)\n",
    "recovered = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv\",keep_default_na=False)\n",
    "\n",
    "confirmed[\"Case_Type\"] = \"Confirmed\"\n",
    "deaths[\"Case_Type\"] = \"Deaths\"\n",
    "recovered[\"Case_Type\"] = \"Recovered\"\n",
    "\n",
    "key_columns = [\"Country/Region\",\n",
    "               \"Province/State\",\n",
    "               \"Lat\",\n",
    "               \"Long\",\n",
    "               \"Case_Type\"]\n",
    "\n",
    "data = [confirmed, deaths, recovered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset stores the number of `Cases` for a given day in columns. \n",
    "This is not useful for reporting, thus we move these date columns to rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(df):\n",
    "    # unpivot all non-key columns\n",
    "    melted = df.melt(id_vars=key_columns, var_name=\"Date\", value_name=\"Cases\")\n",
    "    # change our new Date field to Date type\n",
    "    melted[\"Date\"]= pd.to_datetime(melted[\"Date\"]) \n",
    "    \n",
    "    return melted\n",
    "\n",
    "unpivoted_data = list(map(unpivot, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are recombining the data set first to use county-level data aggregates before 09 March and state-level data thereafter. Because the data for `US-VI` (U.S. Virgin Islands) still contains a comma in data after March 10, we are executing a substitution before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_incorrect_county_state_data(df):\n",
    "    stateBeforeMarch9th = df[ (df[\"Date\"] <= \"2020-03-09\") & (df[\"Country/Region\"] == \"US\") & (df[\"Province/State\"].str.contains(\",\") == False) ].index\n",
    "    countyAfterMarch10th = df[ (df[\"Date\"] > \"2020-03-09\") & (df[\"Country/Region\"] == \"US\") & df[\"Province/State\"].str.contains(\",\") ].index\n",
    "\n",
    "    return df.drop(stateBeforeMarch9th).drop(countyAfterMarch10th)\n",
    "\n",
    "unpivoted_data = [df.replace({\"Virgin Islands, U.S.\": \"Virgin Islands\"}) for df in unpivoted_data]\n",
    "unpivoted_data = [drop_incorrect_county_state_data(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize data on the Virgin Islands and Washington D.C. in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "locality_replacements = {\"Washington, D.C.\": \"District of Columbia\",\n",
    "                         \"Virgin Islands, U.S.\": \"Virgin Islands, VI\"}\n",
    "\n",
    "def replace_localities(df):\n",
    "    return df.replace(locality_replacements)\n",
    "\n",
    "unpivoted_data = [replace_localities(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we resolve the geographies of U.S. counties to their respective states (`Province/State`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivisions = {i.name: i.code for i in pycountry.subdivisions.get(country_code=\"US\")}\n",
    "abbreviations = {subdivisions[k]: k for k in subdivisions}\n",
    "\n",
    "def resolve_US_geography(row):\n",
    "    county, state = row[\"Province/State\"].split(\", \")\n",
    "    state.replace(\"D.C.\", \"DC\")\n",
    "    row[\"Province/State\"] = abbreviations[\"US-\" + state.strip()]\n",
    "    return row\n",
    "        \n",
    "def resolve_geography_df(df):\n",
    "    return df.apply(lambda row: resolve_US_geography(row) if row[\"Country/Region\"] == \"US\" and row[\"Province/State\"] not in list(subdivisions.keys()) and \", \" in row[\"Province/State\"] else row, axis=\"columns\")\n",
    "\n",
    "unpivoted_data = [resolve_geography_df(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of states have inconsistent naming or special characters, such as `Taiwan*`. These are normalised through a replacement `dict` with ISO3166-1 compliant names. Data is then aggregated for each division by date and case type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_names = {\n",
    "    \"Holy See\": \"Holy See (Vatican City State)\",\n",
    "    \"Vatican City\": \"Holy See (Vatican City State)\",\n",
    "    \"Hong Kong SAR\": \"Hong Kong\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran, Islamic Republic of\",\n",
    "    \"Iran\": \"Iran, Islamic Republic of\",\n",
    "    \"Macao SAR\": \"Macao\",\n",
    "    \"Macau\": \"Macao\",\n",
    "    \"Republic of Korea\": \"Korea, Republic of\",\n",
    "    \"South Korea\": \"Korea, Republic of\",\n",
    "    \"Korea, South\": \"Korea, Republic of\",\n",
    "    \"Republic of Moldova\": \"Moldova, Republic of\",\n",
    "    \"Russia\": \"Russian Federation\",\n",
    "    \"Saint Martin\": \"Sint Maarten (Dutch part)\",\n",
    "    \"St. Martin\": \"Sint Maarten (Dutch part)\",\n",
    "    \"Taipei and environs\": \"Taiwan, Province of China\",\n",
    "    \"Vietnam\": \"Viet Nam\",\n",
    "    \"occupied Palestinian territory\": \"Palestine, State of\",\n",
    "    \"Taiwan*\": \"Taiwan, Province of China\",\n",
    "    \"Congo (Brazzaville)\": \"Congo\",\n",
    "    \"Congo (Kinshasa)\": \"Congo, The Democratic Republic of the\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Tanzania\": \"Tanzania, United Republic of\",\n",
    "    \"US\": \"United States\",\n",
    "    \"Curacao\": \"Curaçao\",\n",
    "    \"Brunei\": \"Brunei Darussalam\",\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Moldova\": \"Moldova, Republic of\",\n",
    "    \"The Bahamas\": \"Bahamas\",\n",
    "    \"Venezuela\": \"Venezuela, Bolivarian Republic of\",\n",
    "    \"Bolivia\": \"Bolivia, Plurinational State of\"\n",
    "}\n",
    "\n",
    "\n",
    "for idx, df in enumerate(unpivoted_data):\n",
    "    df[\"Country/Region\"] = df[\"Country/Region\"].replace(changed_names)\n",
    "    df[\"Cases\"] = df[\"Cases\"].replace('',0).astype(int)\n",
    "        \n",
    "    unpivoted_data[idx] = df.groupby(by=[\"Country/Region\",\"Province/State\",\"Date\",\"Case_Type\"], as_index=False) \\\n",
    "        .agg({\"Cases\": \"sum\", \"Long\": \"first\", \"Lat\": \"first\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ISO3166-1 and ISO3166-2 identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate easy recognition, ISO3166-1 identifiers are added to all countries and ISO3166-2 identifiers are added where appropriate. This is the case where subregional data exists:\n",
    "\n",
    "* Australia\n",
    "* Canada\n",
    "* France (`France` for metropolitan France, separate regions for DOM/TOMs\n",
    "* PRC\n",
    "* US\n",
    "* UK (the `UK` province identifier encompasses only Great Britain and Northern Ireland, other dependencies reporting to the UK authorities are separate subdivisions)\n",
    "* The Kingdom of the Netherlands (`Netherlands` encompasses the constituent country of the Netherlands, and the other constituent countries register cases as separate provinces of the Kingdom of the Netherlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_iso3166_1_row(row):\n",
    "    if row[\"Country/Region\"] is not \"Cruise Ship\":\n",
    "        if pycountry.countries.get(name=row[\"Country/Region\"]):\n",
    "            row[\"ISO3166-1\"] = pycountry.countries.get(name=row[\"Country/Region\"]).alpha_2\n",
    "        else:\n",
    "            row[\"ISO3166-1\"] = \"\"\n",
    "    return row\n",
    "\n",
    "def resolve_iso3166_1_df(df):\n",
    "    return df.apply(resolve_iso3166_1_row, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_data = [resolve_iso3166_1_df(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_subdivisions = {\"France\": \"FR\",\n",
    "                       \"French Guiana\": \"GF\",\n",
    "                       \"French Polynesia\": \"PF\",\n",
    "                       \"Guadeloupe\": \"GUA\",\n",
    "                       \"Mayotte\": \"YT\",\n",
    "                       \"Reunion\": \"RE\",\n",
    "                       \"Saint Barthelemy\": \"BL\",\n",
    "                       \"St Martin\": \"MF\"}\n",
    "\n",
    "nl_subdivisions = {\"Netherlands\": \"NL\",\n",
    "                   \"Aruba\": \"AW\",\n",
    "                   \"Curacao\": \"CW\"}\n",
    "\n",
    "cn_subdivisions = {'Jilin': 'CN-JL',\n",
    " 'Xizang': 'CN-XZ',\n",
    " 'Anhui': 'CN-AH',\n",
    " 'Jiangsu': 'CN-JS',\n",
    " 'Yunnan': 'CN-YN',\n",
    " 'Beijing': 'CN-BJ',\n",
    " 'Jiangxi': 'CN-JX',\n",
    " 'Zhejiang': 'CN-ZJ',\n",
    " 'Chongqing': 'CN-CQ',\n",
    " 'Liaoning': 'CN-LN',\n",
    " 'Fujian': 'CN-FJ',\n",
    " 'Guangdong': 'CN-GD',\n",
    " 'Inner Mongolia': 'CN-NM',\n",
    " 'Gansu': 'CN-GS',\n",
    " 'Ningxia': 'CN-NX',\n",
    " 'Guangxi': 'CN-GX',\n",
    " 'Qinghai': 'CN-QH',\n",
    " 'Guizhou': 'CN-GZ',\n",
    " 'Sichuan': 'CN-SC',\n",
    " 'Henan': 'CN-HA',\n",
    " 'Shandong': 'CN-SD',\n",
    " 'Hubei': 'CN-HB',\n",
    " 'Shanghai': 'CN-SH',\n",
    " 'Hebei': 'CN-HE',\n",
    " 'Shaanxi': 'CN-SN',\n",
    " 'Hainan': 'CN-HI',\n",
    " 'Shanxi': 'CN-SX',\n",
    " 'Tianjin': 'CN-TJ',\n",
    " 'Heilongjiang': 'CN-HL',\n",
    " 'Hunan': 'CN-HN',\n",
    " 'Xinjiang': 'CN-XJ',\n",
    " 'Tibet': \"CN-XZ\"}\n",
    "\n",
    "uk_subdivisions = {\"United Kingdom\": \"UK\",\n",
    "                   \"Cayman Islands\": \"KY\",\n",
    "                   \"Channel Islands\": \"CHA\",\n",
    "                   \"Gibraltar\": \"GI\",\n",
    "                   \"Montserrat\": \"MS\"}\n",
    "\n",
    "subdivisions = {\n",
    "    \"AU\": {subdivision.name: subdivision.code.replace(\"AU-\", \"\") for subdivision in pycountry.subdivisions.get(country_code=\"AU\")},\n",
    "    \"CA\": {subdivision.name: subdivision.code.replace(\"CA-\", \"\") for subdivision in pycountry.subdivisions.get(country_code=\"CA\")},\n",
    "    \"US\": {subdivision.name: subdivision.code.replace(\"US-\", \"\") for subdivision in pycountry.subdivisions.get(country_code=\"US\")},\n",
    "    \"GB\": uk_subdivisions,\n",
    "    \"CN\": cn_subdivisions,\n",
    "    \"NL\": nl_subdivisions,\n",
    "    \"FR\": fr_subdivisions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_with_subdivisions = list(subdivisions.keys())\n",
    "\n",
    "def resolve_iso3166_2_row(row):\n",
    "    if row[\"ISO3166-1\"] in countries_with_subdivisions:\n",
    "        row[\"ISO3166-2\"] = subdivisions[row[\"ISO3166-1\"]].get(row[\"Province/State\"])\n",
    "    else:\n",
    "        row[\"ISO3166-2\"] = \"\"\n",
    "    return row\n",
    "\n",
    "def resolve_iso3166_2_df(df):\n",
    "    return df.apply(resolve_iso3166_2_row, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_data = [resolve_iso3166_2_df(df) for df in unpivoted_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating case changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the data by primary keys and `Date` to ensure we can add a `Differences` column as a window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = list(map(lambda df: df.sort_values(by=key_columns + [\"Date\"], ascending=True), unpivoted_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `Cases` are actual snapshots (running numbers), we define changes as the difference to the previous day's value. In other words, `Difference` equals today's `Cases` minus yesterday's `Cases` for each region/state and each case category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in sorted_data:\n",
    "    df[\"Difference\"] = df[\"Cases\"] - df.groupby( key_columns )[\"Cases\"].shift(1, fill_value = 0) \n",
    "\n",
    "concated = pd.concat(sorted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating active cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acive cases are confirmed cases that are not deceased or registered as recovered. This is relevant as active cases determine the demands on the healthcare system. We calculate `Active` case types as:\n",
    "\n",
    "```\n",
    "Active = Confirmed - Deaths - Recovered\n",
    "```\n",
    "\n",
    "As a first step, we merge the different type of cases into a single row for each `Country/Province/Date` keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = concated[concated[\"Case_Type\"].eq(\"Confirmed\")]\n",
    "deaths = concated[concated[\"Case_Type\"].eq(\"Deaths\")]\n",
    "recovered = concated[concated[\"Case_Type\"].eq(\"Recovered\")]\n",
    "\n",
    "active = confirmed  \\\n",
    "        .merge(deaths, validate= \"one_to_one\", suffixes =[\"\",\"_d\"], on=[\"Country/Region\",\"Province/State\",\"Date\"]) \\\n",
    "        .merge(recovered, validate= \"one_to_one\", suffixes =[\"\",\"_r\"], on= [\"Country/Region\",\"Province/State\",\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply the calculations both for `Cases` and `Difference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active[\"Case_Type\"] = \"Active\"\n",
    "active[\"Cases\"] = active[\"Cases\"] - active[\"Cases_r\"] - active[\"Cases_d\"]\n",
    "active[\"Difference\"] = active[\"Difference\"] - active[\"Difference_r\"] - active[\"Difference_d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the `Active` segment with the original one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([concated,active], join=\"inner\")\n",
    "\n",
    "data[\"Case_Type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save the file locally, we add the `Last_Update_Date` in `UTC` time zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Last_Update_Date\"] = datetime.utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the output in the `output` folder as `JHU_COVID-19.csv` as an unindexed CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(output_folder + \"JHU_COVID-19.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
